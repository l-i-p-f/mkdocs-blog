
## How to avoid machine learning pitfalls: a guide for academic researchers

该[论文]((https://arxiv.org/pdf/2108.02497.pdf))主要介绍了机器学习学术研究中的一些常犯错误，以及如何避免。

虽然标题说是给学术研究人员的指南，但从内容看感觉可以当成从事机器学习的一个简单的方法论。或者说一个MLer应该具备的基本素养。

## 如何避免机器学习陷阱

1. **建模前**

 	a. 确保**数据集来源**可靠及**质量**保证

   b. 保证**数据量足够大**。数据少的解决方法：

   	- 交叉验证
   	- 数据扩充
   	- 模型复杂度不要太高，防止过拟合

   c. **先不要看测试数据**，防止先入为主

   d. **搜遍相关文献**，查看相关研究

   e. 事先**考虑模型部署**，落地的资源、时间限制等

2. **建模**

	a. **不要在训练过程使用测试数据**

   b. 尝试不同的ML模型，不套用，**具体问题具体分析**找出最适合解决问题的那个。不合适的例子：

   - 将期望分类特征的模型应用于由数字特征组成的数据集
   - 将假定变量之间没有依赖关系的模型应用于时间序列数据
   - 只相信最新的模型

   c. 使用某种**超参数优化**策略优化模型的超参数

   - AutoML
   - 网格搜索…

   d. 超参数优化和特征选择时要防止测试集泄漏，不要在模型训练开始前对整个数据集进行特征选择。

   - **嵌套交叉验证**

3. **模型评估**

	a. **保证测试集与训练集之间的独立**

   b. **使用验证集指导模型迭代**，不要让测试集参与进来

   c. 对模型多次评估，防止低估/高估性能。**十次交叉验证**是最标准的做法。

   - 某些数据类很小时，需要进行分层（stratification），确保每个类在每次交叉验证中得到充分表示
   - 需要报告多个评估的平均值和标准差
   - 建议保留所有的单个分数记录，以防以后需要统计测试来比较模型

   d. **保留一些测试数据以无偏评估最终的模型实例**

   e. **选择合适的评价指标**

   - 如不要对不平衡数据集使用准确度（accuracy）指标。这个指标常用于分类模型，不平衡数据集应采用kappa系数或马修斯相关系数（MCC）指标。

4. **公平地比较模型**

	a. 应将每个模型**优化到同等程度**，进行**多次评估**，然后使用统计测试确定性能差异是否显著。

   b. 要想让人相信你的模型好，一定要做**统计测试**。

   c. 进行多重比较时进行校正：如果你以95%的置信水平做20个成对测试，其中一个可能会给你错误的答案。这被称为多重性效应。最常见的解决方法是**Bonferroni校正。**

   d. 不要盲目相信公共基准（benchmarks）测试的结果

   - 不清楚别人是否使用测试集数据

   e. 考虑**组合模型**

5. **报告结果**

	a. 保证结果透明，共享代码

   b. 提供**多个测试集**上的报告，为每个数据集报告**多个度量指标**

   c. 不要在结果之外泛化，不夸大，意识到数据的局限性

   d. 最后思考模型**解决了多少实际问题**
